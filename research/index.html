<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> research | Terrence Alsup </title> <meta name="author" content="Terrence Alsup"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://terrencealsup.github.io/research/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Terrence</span> Alsup </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/code/">code </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">research <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <article> <h1 class="post-title">Research</h1> <p>My Ph.D. research was primarily focused on multi-fidelity methods for speeding up inference in uncertainty quantification (UQ) with applications to scientific computing and machine learning.</p> <h1 class="post-title">Introduction</h1> <p>Many problems across computational science, engineering, and machine learning involve the use of a model \(G: \Theta \to \mathcal{Y}\) for some underlying process that relates model parameters \(\theta\) to observables \(\mathbf{y}\). For example, \(\theta\) could be the thermal conductivity of a metal plate and \(\mathbf{y}\) the temperature measurements at different fixed points throughout the plate. The model \(G\), also referred to as the “forward model” or “parameter-to-observable map” depending on the context, maps the thermal conductivity to the temperature at the observation points (assuming appropriate boundary conditions) and requires solving an underlying partial differential equation (PDE). In the case of this example, the heat equation. Except for in special cases where the underlying differential equation has an analytical solution, evaluating the forward model \(G\) may not be possible. Instead, a high-fidelity numerical model \(G^{(L)}:\Theta \to \mathcal{Y}\) is used to approximate the true model \(G\), with the superscript \(L\) denoting the accuracy or fidelity of the model.</p> <p>While the high-fidelity model can often be made arbitarily accurate, for example by refining the size of the grid used to discrtize the PDE, evaluation may incur a significant computational cost (e.g. measured in runtime, memory use, dollars, energy consumption, etc.). However, many ubiquitous tasks in computational science and engineering applications require evaluating the high-fidelity model repeatedly for different inputs. These tasks are referred to as “outer-loop” applications and can still be computationally intractable if the cost of evaluating the high-fidelity model is significant. Examples of such outer-loop applications include:</p> <ul> <li> <strong>Optimization</strong>: Many optimization methods iteratively update a set of design variables to minimize an objective function that depends on the high-fidelity model by evaluating its derivatives.</li> <li> <strong>Inference</strong>: Inferring a quantity of interest, e.g. the mean or variance, from a probability distribution via Monte Carlo methods typically requires repeated evaluations of the log density or its gradient to draw samples.</li> <li> <strong>Data assimilation</strong>: Filtering-based methods that integrate observational data into existing models typically alternate between a prediction step where the model is eval- uated and an update step where the model parameters are adjusted to reflect the new data.</li> <li> <strong>Control</strong>: Controlling a system to a desired state with feedback requires monitoring the system by evaluating the model for the system and adjusting the control variables accordingly.</li> </ul> <p>The simplest approach to circumvent these high computational costs is to simply use a cheaper, but less accurate, numerical model. In other words, <em>replacing</em> the high-fidelity model with a surrogate model. Typical examples of surrogate models used in practice are shown below.</p> <div style="overflow: auto;width: 100%;"> <figure class="right" style="float:left"> <figcaption><b>course grid approximations</b></figcaption> <img src="../assets/img/research/multigrid.png" width="40%"> </figure> <figure class="right" style="float:right"> <figcaption><b>machine learning and data-fit models</b></figcaption> <img src="../assets/img/research/DataFitExample-crop.pdf" width="75%"> </figure> </div> <p><br></p> <div style="overflow: auto;width: 100%;"> <figure class="right" style="float:left"> <figcaption><b>reduced order models</b></figcaption> <img src="../assets/img/research/manifold.pdf" width="200"> </figure> <figure class="right" style="float:right"> <figcaption><b>simplified physics and linearized models</b></figcaption> <img src="../assets/img/research/dnslesrans.jpeg" width="250"> </figure> </div> <p><br></p> <p>While more tractable, replacing the high-fidelity model with a surrogate model introduces an error in final outer-loop result (for example by minimizing a surrogate objective function or sampling from a surrogate density).</p> <p>Multi-fidelity methods <a class="citation" href="#PWG17MultiSurvey">(Peherstorfer et al., 2018)</a> are designed to use both the high-fidelity and surrogate models together. Good multi-fidelity methods will maintain the accuracy of the high-fidelity model while offloading the bulk of the computation in the outer-loop to cheaper surrogate models in order to achieve speedups and reduce overall cost. A special case of multi-fidelity methods is when the surrogate models form a clear hierarchy, such as in the case of nested grids or reduced order models, and is referred to as multilevel methods. The sections that follow focus specifically on multi-fidelity methods for inference in uncertainty quantification.</p> <h1 id="multi-fidelity-methods-for-uq">Multi-fidelity methods for UQ</h1> <p>Generally speaking there are two flavors of uncerainty quantification depending on which quantity, either the parameters \(\theta\) or the observables \(\mathbf{y}\), is treated as the source of randomness.</p> <ul> <li> <strong>Forward UQ</strong> treats the underlying parameters as random and is concerned with the distribution of the model output \(G^{(L)}(\theta)\).</li> <li> <strong>Inverse UQ</strong> models the observed data \(\mathbf{y}\) as random and seeks to infer the corresponding parameters \(\theta\), typically through sampling the posterior distribution \(p(\theta \mid \mathbf{y})\).</li> </ul> <h2 id="forward-uq"><strong>Forward UQ</strong></h2> <p>For this setting, let \(\theta \sim \pi_0\) and \(\mathbf{y} = G^{(L)}(\theta)\) be distributed according to the pushforward distribution \(\pi^{(L)} = G^{(L)}_{\#}\pi_0\). The goal here is to evaluate statistics of the distribution \(\pi^{(L)}\) such as the mean, covariance, or probabilities (especially rare event probabilities). Because expectation amounts to integration, for high-dimensional parameters the standard approach is to use Monte Carlo methods.</p> <p>Two distinct examples of multi-fidelity methods for forward UQ are the multi-fidelity cross-entropy method (MFCE) <a class="citation" href="#PKW17MFCE">(Peherstorfer et al., 2018)</a> and multi-fidelity control variates (MFCV) <a class="citation" href="#P19AMFMC">(Peherstorfer, 2019)</a>.</p> <h3 id="multi-fidelity-cross-entropy-method">Multi-fidelity cross-entropy method</h3> <p>The multi-fidelity cross-entropy method specializes in estimating rare event probabilities of the form</p> \[p_t = \mathbb{E}\left[ \mathbf{1}\{ G^{(L)}(\theta) &gt; t \} \right] ,\] <p>where \(t \in \mathbb{R}\) denotes some threshold. To estimate \(p_t\), the cross-entropy method searches for a suitable biasing density to be used for importance sampling. Given a family \(\mathcal{P}\) of probability distributions, typically normal distributions, the cross-entropy method starts with an initial biasing density \(q_0 \in \mathcal{P}\) and threshold \(t_0\). The method proceeds by alternating between sampling from and then updating both the threshold and biasing density.</p> \[\begin{align*} \theta_1,\ldots,\theta_n &amp;\sim q_k \\ q_{k+1} &amp;= \underset{q \in \mathcal{P}}{\text{arg min}}\ \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{ G^{(L)}(\theta_i) &gt; t_k \} \frac{\pi^{(L)}(\theta_i)}{q_{k}(\theta_i)} \log q (\theta_i) \\ t_{k+1} &amp;= \rho\text{-quantile}(\theta_1,\ldots,\theta_n) \end{align*}\] <p>where \(\rho\)-quantile denotes the \(\rho\)-th quantile from the samples \(\theta_1,\ldots,\theta_n\). The procedure ends when the threshold \(t_k\) reaches \(t\) and \(q_{k}\) is used as a biasing density for importance sampling to estimate \(p_t\):</p> \[\hat{p}_t = \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{ G^{(L)}(\theta_i) &gt; t \} \frac{\pi^{(L)}(\theta_i)}{q_{k}(\theta_i)} .\] <p>The major drawback of this method is that potentially many iterations will be needed if the initial biasing density is a poor approximation of the optimal biasing density of if the sequence of thresholds is slow to converge. The multi-fidelity cross-entropy method (MFCE) introduced in <a class="citation" href="#PKW17MFCE">(Peherstorfer et al., 2018)</a> instead starts with the cheapest surrogate model \(\pi^{(1)}\) and iterates until the threshold \(t\) is achieved. The final biasing density from the cheapest surrogate model is then passed on as the initial density for the second cheapest surrogate model \(\pi^{(2)}\). This process repeats until we reach the high-fidelity density with the hope that a good initialization has been found for the biasing density and thus only a few iterations will be needed at the most expensive level. A Matlab implementation of this method applied to the problem of inferring a heat conductivity field from temperature measurements on the boundary of the domain can be found in this <a href="https://github.com/terrencealsup/UQ-final-project" rel="external nofollow noopener" target="_blank">repository</a> from a class project with Frederick Law.</p> <h3 id="multi-fidelity-control-variates">Multi-fidelity control variates</h3> <p>The MFCE method uses surrogate densities to find a suitable biasing density for importance sampling and reduce the variance in estimating the rare event probability \(p_t\). The multi-fidelity control variates method (MFCV), on the other hand, takes advantage of the correlation between the high-fidelity and surrogate models.</p> <h3 id="multi-fidelity-covariance-estimation">Multi-fidelity covariance estimation</h3> <p>Control variates work similarly for covariance estimation, however the estimate is not guaranteed to remain positive definite</p> \[\hat{\Sigma}^{\text{MF}} = \hat{\Sigma}^{\text{HF}}_{n} + \alpha \left( \hat{\Sigma}^{\text{HF}}_{m} - \hat{\Sigma}^{\text{LF}}_{n} \right)\] <p>While this estimator is optimal in the sense of minimizing the Frobenius distance, it is not guaranteed to be a positive definite, a required property for many applications of covariance matrics from control to sampling to metric learning.</p> <h2 id="inverse-uq"><strong>Inverse UQ</strong></h2> <p>Inverse UQ is often formulated as a Bayesian or statistical inverse problem. Given some observational data \(\mathbf{y}\) that results from a forward model \(G\), we want to infer model parameters \(\boldsymbol{\theta}\). Typically, this problem is ill-posed with and instead some regularization is required to solve the inverse problem</p> \[\underset{\boldsymbol{\theta}}{\min}\quad \underbrace{\left\lVert G({\boldsymbol{\theta}}) - \mathbf{y} \right \rVert^2 }_{\text{data misfit}} + \underbrace{\lambda \left\lVert \boldsymbol{\theta} \right\rVert^2}_{\text{regularization}}\] <p>The result of solving this expensive minimization problem is a single point-estimate. However, we may be interested in assessing the variance (or more generally the distribution) in the estimate as well. This leads to Bayesian or statistical inverse problems</p> \[p(\boldsymbol{\theta} \mid \mathbf{y}) \propto \exp\left( -\frac{1}{2} \left\lVert G({\boldsymbol{\theta}}) - \mathbf{y} \right \rVert^2 \right) \pi_0(\boldsymbol{\theta})\] <p>For low-dimensional parameters it may be sufficient to integrate directly, use sparse grid techniques, or even quasi-Monte Carlo methods. However, for higher dimensions the only feasbile solution is to use Monte Carlo methods.</p> <h3 id="multilevel-stein-variational-gradient-descent">Multilevel Stein Variational Gradient Descent</h3> <p>Stein variational gradient descent is an interacting particle method for performing inference</p> \[\boldsymbol{\theta}^{[j]}_{t+1} = \frac{1}{n}\sum_{i=1}^n K(\boldsymbol{\theta}^{[i]}_{t}, \boldsymbol{\theta}^{[j]}_{t}) \nabla \log p\] <h3 id="multi-fidelity-importance-sampling">Multi-fidelity Importance Sampling</h3> <h1>References</h1> <div class="publications"> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="P19AMFMC" class="col-sm-8"> <div class="title">Multifidelity Monte Carlo estimation with adaptive low-fidelity models</div> <div class="author"> B. Peherstorfer </div> <div class="periodical"> <em>SIAM/ASA Journal on Uncertainty Quantification</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="PWG17MultiSurvey" class="col-sm-8"> <div class="title">Survey of multifidelity methods in uncertainty propagation, inference, and optimization</div> <div class="author"> B. Peherstorfer, K. Willcox, and M. Gunzburger </div> <div class="periodical"> <em>SIAM Review</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="PKW17MFCE" class="col-sm-8"> <div class="title">Multifidelity preconditioning of the cross-entropy method for rare event simulation and failure probability estimation</div> <div class="author"> B. Peherstorfer, B. Kramer, and K. Willcox </div> <div class="periodical"> <em>SIAM/ASA Journal on Uncertainty Quantification</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Terrence Alsup. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>