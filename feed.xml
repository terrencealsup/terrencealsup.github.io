<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://terrencealsup.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://terrencealsup.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-18T18:57:01+00:00</updated><id>https://terrencealsup.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Building a 2-layer neural network from scratch with NumPy</title><link href="https://terrencealsup.github.io/blog/2023/building-neural-network-from-scratch/" rel="alternate" type="text/html" title="Building a 2-layer neural network from scratch with NumPy"/><published>2023-08-21T00:00:00+00:00</published><updated>2023-08-21T00:00:00+00:00</updated><id>https://terrencealsup.github.io/blog/2023/building-neural-network-from-scratch</id><content type="html" xml:base="https://terrencealsup.github.io/blog/2023/building-neural-network-from-scratch/"><![CDATA[<p>In this post we’ll build a two-layer neural network from scratch in Python using only the <a href="https://numpy.org">NumPy</a> library. The full code implementation as well as the test example and plots are contained in this Jupyter <a href="/assets/posts/two-layer-network/TwoLayerNetwork.ipynb">notebook</a>.</p> <p>A two-layer neural network is a parametric function \(f:\mathbb{R}^{m_0} \to \mathbb{R}^{m_2}\) of the form</p> \[f(\mathbf{x}; \mathbf{W}_1, \mathbf{W}_2, \mathbf{b}_1, \mathbf{b}_2) = \mathbf{W}_2 \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2 \, .\] <p>The network parameters include the matrices \(\mathbf{W}_1 \in \mathbb{R}^{m_1 \times m_0}\), \(\mathbf{W}_2 \in \mathbb{R}^{m_2 \times m_1}\), which are the weights of the network, and the vectors \(\mathbf{b}_1 \in \mathbb{R}^{m_1}\), \(\mathbf{b}_2 \in \mathbb{R}^{m_2}\), which are the biases. Here \(m_1\) denotes the dimension or number nodes in the hidden layer and the function \(\sigma : \mathbb{R}^{m_1} \to \mathbb{R}^{m_1}\) is an element-wise activation function that allows the network to approximate nonlinear functions.</p> <p>Given paired training data</p> \[\texttt{data} = \left\{ \left(\mathbf{x}_i, \mathbf{y}_i \right) \right\}_{i=1}^n\] <p>we want to find parameters \(\mathbf{W}_1,\mathbf{W}_2,\mathbf{b}_1,\mathbf{b}_2\) that minimize the mean-squared error loss function</p> \[\ell\left(\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}_1, \mathbf{b}_2; \{(\mathbf{x}_i, \mathbf{y}_i) \}_{i=1}^n \right) = \frac{1}{n} \sum_{i=1}^n \lVert f(\mathbf{x}_i) - \mathbf{y}_i\rVert^2\, ,\] <p>so as to fit the network to the data.</p> <h3 id="activation-function">Activation function</h3> <p>To finish specifying our network we also need to decide on an activation function. A common choice is the rectified linear unit or ReLU, defined by \(\texttt{relu}(z) = \max(z, 0)\). The ReLU function can easily be implemented using NumPy’s <a href="https://numpy.org/doc/stable/reference/generated/numpy.maximum.html"><code class="language-plaintext highlighter-rouge">maximum</code></a> function, which automatically operates element-wise on NumPy arrays.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Rectified linear activation unit</span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><img src="/assets/posts/two-layer-network/relu.png" alt=""/></p> <p>ReLU is differentiable almost everywhere except at the point 0 at which point we need to use a subdifferential. Here we’ll set the derivative at this point to be 0 so that the derivative is the Heaviside function.</p> \[\texttt{relu}^{\prime}(z) = \begin{cases} 1, &amp; z &gt; 0 \\ 0, &amp; z \le 0 \end{cases}\] <p>Again, this function is easily implemented in NumPy by evaluating a boolean expression at each entry and then casting to a float.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre>    <span class="k">def</span> <span class="nf">grad_relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Heaviside function</span><span class="sh">"""</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="the-network-class">The network class</h3> <p>We’ll implement the two-layer network as a class in Python that contains variables for the parameters as well as functions to predict and train the network. We’ll also add a helper function to compute the loss on the data set as well as evaluate the gradients with respect to the parameters for training. When a network object is instantiated the parameters are initialized to random numbers drawn from a normal distribution.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="code"><pre>    <span class="k">class</span> <span class="nc">TwoLayerNetwork</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">m0</span><span class="p">,</span> <span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">):</span>
            <span class="sh">"""</span><span class="s">Initialize the network parameters.</span><span class="sh">"""</span>
            <span class="n">self</span><span class="p">.</span><span class="n">m0</span> <span class="o">=</span> <span class="n">m0</span>
            <span class="n">self</span><span class="p">.</span><span class="n">m1</span> <span class="o">=</span> <span class="n">m1</span>
            <span class="n">self</span><span class="p">.</span><span class="n">m2</span> <span class="o">=</span> <span class="n">m2</span>
            <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m0</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">m2</span><span class="p">,</span> <span class="n">m1</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">m2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
            <span class="sh">"""</span><span class="s">Evaluate the network on the input X.</span><span class="sh">"""</span>
            <span class="k">pass</span>
        
        <span class="k">def</span> <span class="nf">grad_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="sh">"""</span><span class="s">Compute the loss and gradients w.r.t. parameters.</span><span class="sh">"""</span>
            <span class="k">pass</span>

        <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="sh">"""</span><span class="s">Train the network on the data (X, y).</span><span class="sh">"""</span>
            <span class="k">pass</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="prediction">Prediction</h3> <p>The first method we should implement is <code class="language-plaintext highlighter-rouge">predict</code> since it is needed to evaluate the function \(f\) on a test input. Let’s suppose that our input vector is a column vector \(\mathbf{x} \in \mathbb{R}^{m_0}\), or equivalently a NumPy array of shape (\(m_0\), 1). For the loss function, we will eventually want to make predictions on a batch of input vectors. Given a batch of training points \(\mathbf{x}_1,\ldots,\mathbf{x}_n \in \mathbb{R}^{m_0}\), define the matrix</p> \[\mathbf{X} = \begin{bmatrix} \mathbf{x}_1 &amp; \cdots &amp; \mathbf{x}_n \end{bmatrix} \in \mathbb{R}^{m_0 \times n} \, ,\] <p>so that each input is a column. The output of <code class="language-plaintext highlighter-rouge">predict</code> should then be a Numpy array of shape (\(m_2\), \(n\))</p> \[f(\mathbf{X}) = \begin{bmatrix} f(\mathbf{x}_1) &amp; \cdots &amp; f(\mathbf{x}_n) \end{bmatrix} \in \mathbb{R}^{m_2 \times n} \, ,\] <p>where again each column is the corresponding output. It’s convenient to store all of the data in a matrix \(\mathbf{X}\) because matrix multiplication with \(\mathbf{W}_1\) and \(\mathbf{W}_2\) act directly on each column. For example,</p> \[\mathbf{W}_1\mathbf{X} = \begin{bmatrix} \mathbf{W}_1\mathbf{x}_1 &amp; \cdots &amp; \mathbf{W}_1\mathbf{x}_n \end{bmatrix} \in \mathbb{R}^{m_1 \times n} \, .\] <p>Earlier we also took care to define <code class="language-plaintext highlighter-rouge">b1</code> and <code class="language-plaintext highlighter-rouge">b2</code> as column vectors. This is to ensure that these vectors are correctly added (through NumPy broadcasting) to each column as opposed to the rows in the event that \(m_0 = m_1\).</p> <p>The code below evaluates the network on a batch of inputs \(\mathbf{X}\) and returns the matrix of outputs \(f(\mathbf{X})\).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Evaluate the network on test points.</span><span class="sh">"""</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b1</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">@</span> <span class="nf">relu</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b2</span>
        <span class="k">return</span> <span class="n">y2</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="loss-function-and-gradients">Loss function and gradients</h3> <p>The next method that we need to implement is a method for computing the loss on the training data as well as the gradient of the loss function with respect to the parameters. This will be critical for training the network and monitoring its performance. To simplify notation as much as possible, let’s define the matrices</p> \[\mathbf{Y} = \begin{bmatrix} \mathbf{y}_1 &amp; \cdots &amp; \mathbf{y}_n \end{bmatrix} \in \mathbb{R}^{m_2 \times n}\, ,\quad \mathbf{R} = f(\mathbf{X}) - \mathbf{Y} \, .\] <p>The matrix \(\mathbf{R}\) contains the residuals for each point</p> \[\mathbf{R} = (r_{ji})_{1 \le j \le m_2\\ 1 \le i \le n}\, ,\quad r_{ji} = \left(f(\mathbf{x}_i) - \mathbf{y}_i\right)_j \, ,\] <p>with the subscript \(j\) denoting the row or component of the vector. Because the loss function is the mean-squared error we can write</p> \[\begin{align*} \texttt{sum}(\mathbf{R} \odot \mathbf{R}) &amp;= \sum_{i=1}^n \sum_{j=1}^{m_2} r_{ji}^2 \\ &amp;= \sum_{i=1}^n \lVert f(\mathbf{x}_i) - \mathbf{y}_i \rVert^2 \\ &amp;= n \cdot \ell\left(\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}_1, \mathbf{b}_2; \{(\mathbf{x}_i, \mathbf{y}_i) \}_{i=1}^n \right) \, , \end{align*}\] <p>where \(\odot\) represents element-wise multiplication and \(\texttt{sum}\) is a function that sums all of the entries in the matrix. In code this looks like:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">grad_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Compute the loss and gradients w.r.t. parameters.</span><span class="sh">"""</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">fx</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">R</span> <span class="o">*</span> <span class="n">R</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>To obtain the gradients of the loss function we will compute them explicitly. For more complicated networks it would be better to use an automatic differentiation package such as <a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html">Jax</a> or <a href="https://pytorch.org">PyTorch</a>, but here the problem is simple enough for us to derive the gradients by hand. For the gradients we get</p> <p style="font-size : 16px"> \begin{align*} \nabla_{\mathbf{W}_1} \ell &amp;= \frac{2}{n} \sum_{i=1}^n \left( \mathbf{W}_2^{\top}\left( f(\mathbf{x}_i) - \mathbf{y}_i\right) \odot \sigma^{\prime}(\mathbf{W}_1\mathbf{x}_i + \mathbf{b}_1) \right) \mathbf{x}_i^{\top} \\ \nabla_{\mathbf{W}_2} \ell &amp;= \frac{2}{n}\sum_{i=1}^n \left( f(\mathbf{x}_i) - \mathbf{y}_i \right) \sigma\left( \mathbf{W}_1\mathbf{x}_i + \mathbf{b}_1\right)^{\top} \\ \nabla_{\mathbf{b}_1} \ell &amp;= \frac{2}{n}\sum_{i=1}^n \mathbf{W}_2^{\top}\left(f(\mathbf{x}_i) - \mathbf{y}_i \right) \odot \sigma^{\prime}(\mathbf{W}_1\mathbf{x}_i + \mathbf{b}_1) \\ \nabla_{\mathbf{b}_2} \ell &amp;= \frac{2}{n}\sum_{i=1}^n \left( f(\mathbf{x}_i) - \mathbf{y}_i \right) \, . \end{align*} </p> <p>A detailed derivation of these gradients can be found <a href="/assets/posts/two-layer-network/gradients.pdf">here</a>.</p> <p>To implement these in Python for a batch of training points and to avoid redundant computations define the matrices</p> \[\mathbf{S} = \sigma\left( \mathbf{W}_1\mathbf{X} + \mathbf{b}_1 \mathbf{1}_{n}^{\top} \right)\, ,\quad \mathbf{D} = \sigma^{\prime}\left( \mathbf{W}_1\mathbf{X} + \mathbf{b}_1 \mathbf{1}_{n}^{\top} \right) \, ,\] <p>both of which are \(m_1 \times n\) matrices. Here \(\mathbf{1}_n = [1,\ldots, 1] \in \mathbb{R}^{n}\) is a vector of only ones so that</p> \[\mathbf{b}_1 \mathbf{1}_{n}^{\top} = \begin{bmatrix} \mathbf{b}_1 &amp; \cdots &amp; \mathbf{b}_1 \end{bmatrix} \in \mathbb{R}^{m_1 \times n} \, .\] <p>In Python we don’t actually need to do this because NumPy will automatically broadcast and add the vector \(\mathbf{b}_1\) to each column of the matrix \(\mathbf{W}_1\mathbf{X}\).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="n">S</span> <span class="o">=</span> <span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b1</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="nf">grad_relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>We’ll also define the matrix</p> \[\mathbf{V} = \mathbf{W}_2^{\top}\mathbf{R} \in \mathbb{R}^{m_1 \times n}\, .\] <p>The vectorized implementation of the gradients can be written as</p> <p style="font-size : 16px"> \begin{align*} \nabla_{\mathbf{W}_1} \ell &amp;= \frac{2}{n} \left( \mathbf{V}\odot \mathbf{D} \right) \mathbf{X}^{\top} \\ \nabla_{\mathbf{W}_2} \ell &amp;= \frac{2}{n} \mathbf{R} \mathbf{S}^{\top} \\ \nabla_{\mathbf{b}_1} \ell &amp;= \frac{2}{n}\texttt{sum}\left( \mathbf{V}\odot \mathbf{D},\ \texttt{columns} \right) \\ \nabla_{\mathbf{b}_2} \ell &amp;= \frac{2}{n}\texttt{sum}\left( \mathbf{R},\ \texttt{columns} \right)\, , \end{align*} </p> <p>where the function \(\texttt{sum}\left( \mathbf{A},\ \texttt{columns} \right)\) denotes the vector that is the sum of the columns of the matrix \(\mathbf{A}\).</p> <p>Equivalently with NumPy:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre><span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">W2</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">R</span>
<span class="n">grad_W1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">*</span> <span class="n">D</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>
<span class="n">grad_W2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span> <span class="o">@</span> <span class="n">S</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">grad_b1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">V</span> <span class="o">*</span> <span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grad_b2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The complete <code class="language-plaintext highlighter-rouge">grad_loss</code> method is below. Note that after computing the gradients <code class="language-plaintext highlighter-rouge">grad_b1</code> and <code class="language-plaintext highlighter-rouge">grad_b2</code> we reshape the Numpy arrays to column vectors to ensure that they are broadcasted correctly. We also use Numpy’s <code class="language-plaintext highlighter-rouge">mean</code> function as opposed to the <code class="language-plaintext highlighter-rouge">sum</code> function and then dividing by \(n\), in case of potential overflow.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">grad_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Compute the gradient of the loss on the training set.</span><span class="sh">"""</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Number of training samples
</span>    <span class="n">fx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># shape (m2, n)
</span>    
    <span class="c1"># Pre-compute matrices
</span>    <span class="n">R</span> <span class="o">=</span> <span class="n">fx</span> <span class="o">-</span> <span class="n">y</span> <span class="c1"># residuals
</span>    <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">W2</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">R</span>
    <span class="n">S</span> <span class="o">=</span> <span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b1</span><span class="p">)</span> <span class="c1"># shape (m1, n)
</span>    <span class="n">D</span> <span class="o">=</span> <span class="nf">grad_relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b1</span><span class="p">)</span> <span class="c1"># shape (m1, n)
</span>    
    <span class="c1"># Multiply the loss by m2 since there are n*m2 elements
</span>    <span class="c1"># in the matrices fx, y
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">m2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">R</span><span class="o">*</span><span class="n">R</span><span class="p">)</span>
    
    <span class="c1"># Gradients
</span>    <span class="n">grad_W1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">*</span> <span class="n">D</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>
    
    <span class="n">grad_W2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span> <span class="o">@</span> <span class="n">S</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    
    <span class="n">grad_b1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">V</span> <span class="o">*</span> <span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">grad_b1</span> <span class="o">=</span> <span class="n">grad_b1</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">m1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">grad_b2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">grad_b2</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">m2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="training">Training</h3> <p>Now that the gradients are implemented we can use gradient descent to train the network. We will keep the training method as simple by specifying a fixed learning rate and a fixed number of epochs. We will also use full gradient descent as opposed to stochastic gradient descent or minibatches.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Train the network using gradient descent.</span><span class="sh">"""</span>   
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>  
        <span class="c1"># Compute loss function and gradients
</span>        <span class="n">loss</span><span class="p">,</span> <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span> <span class="o">=</span> \
        <span class="n">self</span><span class="p">.</span><span class="nf">grad_loss</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>    
        
        <span class="c1"># Update parameters
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">grad_W1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">grad_W2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">b1</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">grad_b1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">b2</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">grad_b2</span>        
    <span class="k">return</span> <span class="n">loss</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="completed-network-class">Completed network class</h3> <p>Note that in the <code class="language-plaintext highlighter-rouge">train</code> method we also use the <a href="https://tqdm.github.io"><code class="language-plaintext highlighter-rouge">tqdm</code></a> package to track the optimizer’s progress.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>


<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Rectified linear unit
    
    Input:
    x - numpy array of shape (m, n) where n is the number
        of points and m is the dimension
    
    Return:
    numpy array of shape (m ,n) of the pointwise ReLU
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">grad_relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Heaviside function
    
    Input:
    x - numpy array of shape (m, n) where n is the number
        of points and m is the dimension
    
    Return:
    numpy array of shape (m ,n) of the pointwise Heaviside
    </span><span class="sh">"""</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TwoLayerNetwork</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">m0</span><span class="p">,</span> <span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Initialize the network with random weights.
        
        Input:
        m0 - int for the input dimension
        m1 - int for the hidden layer dimension
        m2 - int for the output dimension
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">m0</span> <span class="o">=</span> <span class="n">m0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">m1</span> <span class="o">=</span> <span class="n">m1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">m2</span> <span class="o">=</span> <span class="n">m2</span>
        <span class="c1"># Weight matrices
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">m2</span><span class="p">,</span> <span class="n">m1</span><span class="p">)</span>
        <span class="c1"># Save b1, b2 as column vectors for correct
</span>        <span class="c1"># broadcasting during prediction.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">m2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
   

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Evaluate the network on test points.
        
        Input:
        X - numpy array of shape (m0, n) where m0 is the input
            dimension and n is the number of points
        
        Return:
        y2 - numpy array of shape (m2, n) where m2 is the 
             output dimension of the network
        </span><span class="sh">"""</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b1</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">@</span> <span class="nf">relu</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b2</span>
        <span class="k">return</span> <span class="n">y2</span>
     
        
    <span class="k">def</span> <span class="nf">grad_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Compute the gradient of the loss on the training set.
        
        The loss is given by
        1/n sum_{i=1}^n ||f(x_i) - y_i||^2
        
        Input:
        X - numpy array of shape (m0, n)
        y - numpy array of shape (m2, n)
        
        Return:
        loss - float for the mean-squared error of the network on 
               the training points
        grad_W1 - numpy array of shape (m0, m1)
        grad_W2 - numpy array of shape (m1, m2)
        grad_b1 - numpy array of shape (m1, 1)
        grad_b2 - numpy array of shape (m2, 1)
        </span><span class="sh">"""</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Number of training samples
</span>        <span class="n">fx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># shape (m2, n)
</span>        
        <span class="c1"># Pre-compute matrices
</span>        <span class="n">R</span> <span class="o">=</span> <span class="n">fx</span> <span class="o">-</span> <span class="n">y</span> <span class="c1"># residuals
</span>        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">W2</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">R</span>
        <span class="n">S</span> <span class="o">=</span> <span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b1</span><span class="p">)</span> <span class="c1"># shape (m1, n)
</span>        <span class="n">D</span> <span class="o">=</span> <span class="nf">grad_relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b1</span><span class="p">)</span> <span class="c1"># shape (m1, n)
</span>        
        <span class="c1"># Multiply the loss by m2 since there are n*m2 elements
</span>        <span class="c1"># in the matrices fx, y
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">m2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">R</span><span class="o">*</span><span class="n">R</span><span class="p">)</span>
        
        <span class="c1"># Gradients
</span>        <span class="n">grad_W1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">*</span> <span class="n">D</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>
        
        <span class="n">grad_W2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span> <span class="o">@</span> <span class="n">S</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        
        <span class="n">grad_b1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">V</span> <span class="o">*</span> <span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">grad_b1</span> <span class="o">=</span> <span class="n">grad_b1</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">m1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="n">grad_b2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">grad_b2</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">m2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span>
       
        
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Train the network using gradient descent.
        Updates the network parameters in place.
        
        Input:
        X - numpy array of shape (m0, n)
        y - numpy array of shape (m2, n)
        lr - float &gt; 0 that specifies the learning rate
        epochs - int &gt; 0 that specifies the number of iterations
        
        Return:
        loss - float for the final mean-squared error of the 
               network on the training points
        </span><span class="sh">"""</span>   
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>  
            <span class="c1"># Compute loss function and gradients
</span>            <span class="n">loss</span><span class="p">,</span> <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span> <span class="o">=</span> \
            <span class="n">self</span><span class="p">.</span><span class="nf">grad_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>    
            
            <span class="c1"># Update parameters
</span>            <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">grad_W1</span>
            <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">grad_W2</span>
            <span class="n">self</span><span class="p">.</span><span class="n">b1</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">grad_b1</span>
            <span class="n">self</span><span class="p">.</span><span class="n">b2</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">grad_b2</span>        
        <span class="k">return</span> <span class="n">loss</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="test-example">Test example</h3> <p>Let’s now try out our implementation on a toy problem where we want to approximate the ground truth function \(f^{*}(x) = x^2\). We’ll generate a dataset of 1000 training and test examples and use a network with 15 hidden nodes. For training we’ll use a learning rate <code class="language-plaintext highlighter-rouge">lr = 1e-3</code> and <code class="language-plaintext highlighter-rouge">epochs = 2500</code>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre><span class="c1"># Instantiate network with 15 hidden nodes
</span><span class="n">nn</span> <span class="o">=</span> <span class="nc">TwoLayerNetwork</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Generate training and test data
</span><span class="n">ntrain</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">Xtrain</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntrain</span><span class="p">)</span>
<span class="n">ytrain</span> <span class="o">=</span> <span class="n">Xtrain</span> <span class="o">*</span> <span class="n">Xtrain</span>

<span class="n">ntest</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">Xtest</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">ntest</span><span class="p">).</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntest</span><span class="p">))</span>
<span class="n">ytest</span> <span class="o">=</span> <span class="n">Xtest</span> <span class="o">*</span> <span class="n">Xtest</span>

<span class="c1"># Train the network
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mi">2500</span><span class="p">)</span>

<span class="c1"># Get the prediction on the test data
</span><span class="n">ypred</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The results are shown below. We indeed see that both the training and validation loss decrease over time and that the validation loss is only slightly higher than the training loss. For the plot on the right we see the predictions from the test data closely match the ground truth function. Code to generate this plot and run this example can be found in this <a href="/assets/posts/two-layer-network/TwoLayerNetwork.ipynb">notebook</a>.</p> <p><img src="/assets/posts/two-layer-network/test_example_loss.png" alt=""/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post we’ll build a two-layer neural network from scratch in Python using only the NumPy library. The full code implementation as well as the test example and plots are contained in this Jupyter notebook.]]></summary></entry></feed>